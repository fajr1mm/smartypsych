{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7103189,"sourceType":"datasetVersion","datasetId":4094763},{"sourceId":7121592,"sourceType":"datasetVersion","datasetId":4107722},{"sourceId":7185994,"sourceType":"datasetVersion","datasetId":4154292}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Package & Dataset","metadata":{"_uuid":"7052f9d8-8e4a-4f45-a3b3-6b5c956d064d","_cell_guid":"436aed8b-6d5c-4fef-ae08-de357d2e871c","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T09:01:19.626319Z","iopub.execute_input":"2023-12-02T09:01:19.626712Z","iopub.status.idle":"2023-12-02T09:01:19.631408Z","shell.execute_reply.started":"2023-12-02T09:01:19.626685Z","shell.execute_reply":"2023-12-02T09:01:19.630426Z"}}},{"cell_type":"code","source":"import tensorflow as tf\n\ndevice_name = tf.test.gpu_device_name()\n\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\n    \nprint('Found GPU at: {}'.format(device_name))\n\nprint(\"GPU\", \"available (YESS!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:30:46.141985Z","iopub.execute_input":"2023-12-13T12:30:46.142812Z","iopub.status.idle":"2023-12-13T12:30:46.156513Z","shell.execute_reply.started":"2023-12-13T12:30:46.142780Z","shell.execute_reply":"2023-12-13T12:30:46.155605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport glob\nfrom datasets import load_dataset\nimport datasets","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:30:46.158276Z","iopub.execute_input":"2023-12-13T12:30:46.158575Z","iopub.status.idle":"2023-12-13T12:30:46.166872Z","shell.execute_reply.started":"2023-12-13T12:30:46.158550Z","shell.execute_reply":"2023-12-13T12:30:46.165932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import langid","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:30:46.168146Z","iopub.execute_input":"2023-12-13T12:30:46.168430Z","iopub.status.idle":"2023-12-13T12:30:46.177153Z","shell.execute_reply.started":"2023-12-13T12:30:46.168399Z","shell.execute_reply":"2023-12-13T12:30:46.176255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:30:46.178308Z","iopub.execute_input":"2023-12-13T12:30:46.178572Z","iopub.status.idle":"2023-12-13T12:30:58.295258Z","shell.execute_reply.started":"2023-12-13T12:30:46.178550Z","shell.execute_reply":"2023-12-13T12:30:58.294035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import T5Tokenizer, DataCollatorForSeq2Seq\nfrom transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom datasets import concatenate_datasets","metadata":{"_uuid":"544761f1-cda4-4e90-9920-4db35b4e7bb3","_cell_guid":"34bd1cc1-b291-4e29-b5bc-46f330300d22","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:30:58.298261Z","iopub.execute_input":"2023-12-13T12:30:58.298586Z","iopub.status.idle":"2023-12-13T12:30:58.304312Z","shell.execute_reply.started":"2023-12-13T12:30:58.298555Z","shell.execute_reply":"2023-12-13T12:30:58.303399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/praproses-responses-csv/praproses_responses.csv')\n# pd.set_option('display.max_colwidth', None)\nprint(df.dtypes)\ndf","metadata":{"_uuid":"7b3b67f5-4fcf-4598-845c-ffa2fc306a54","_cell_guid":"a4d77e6f-2b60-4c54-9c84-435049575e95","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:30:58.305426Z","iopub.execute_input":"2023-12-13T12:30:58.305683Z","iopub.status.idle":"2023-12-13T12:30:58.341518Z","shell.execute_reply.started":"2023-12-13T12:30:58.305661Z","shell.execute_reply":"2023-12-13T12:30:58.340528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['translated'] = df['answer'].apply(translate_text)\ndf['LEVEL KOMPETENSI'] = df['LEVEL KOMPETENSI'].astype(str)","metadata":{"_uuid":"e5ddb1ca-66ab-4ef2-8a89-933aa081d7a9","_cell_guid":"09e14161-6a26-4f17-b76a-9dbe18de3119","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:30:58.343121Z","iopub.execute_input":"2023-12-13T12:30:58.343662Z","iopub.status.idle":"2023-12-13T12:30:58.349655Z","shell.execute_reply.started":"2023-12-13T12:30:58.343625Z","shell.execute_reply":"2023-12-13T12:30:58.348583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAIN/TEST SPLIT","metadata":{"_uuid":"bbd1395b-eb54-4b16-adbf-c402d7d40543","_cell_guid":"f0c0151a-6f5d-4174-986e-76d6a5748330","trusted":true}},{"cell_type":"code","source":"# yahoo_answers_qa = yahoo_answers_qa[\"train\"].train_test_split(test_size=0.3)\nfrom sklearn.model_selection import train_test_split\n\n# Membagi dataset menjadi data train (70%) dan data test (30%)\ndf_train, df_test = train_test_split(df, test_size=0.3, random_state=42)\n\ndf_train","metadata":{"_uuid":"dd73d595-b3e6-43a3-80f0-483c007161ab","_cell_guid":"0dff8a78-0d6a-4dc0-af00-fc40f35e6f52","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:30:58.351084Z","iopub.execute_input":"2023-12-13T12:30:58.351370Z","iopub.status.idle":"2023-12-13T12:30:58.367326Z","shell.execute_reply.started":"2023-12-13T12:30:58.351345Z","shell.execute_reply":"2023-12-13T12:30:58.366409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\n\n# Misalkan X_train dan X_test adalah DataFrame dengan kolom 'id', 'answer', 'label', 'translated'\n# Pilih kolom-kolom yang ingin Anda gunakan\n\n# Ubah DataFrame menjadi dataset dengan menggunakan Dataset dari Hugging Face's transformers\ntrain_dataset = Dataset.from_pandas(df_train)\ntest_dataset = Dataset.from_pandas(df_test)\n\n# Membuat DatasetDict dengan format yang diinginkan\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"test\": test_dataset,\n})\n\n# Menghapus kolom '__index_level_0__' dari \"train\" dataset\ndataset[\"train\"] = dataset[\"train\"].remove_columns('__index_level_0__')\n\n# Menghapus kolom '__index_level_0__' dari \"test\" dataset\ndataset[\"test\"] = dataset[\"test\"].remove_columns('__index_level_0__')\n\n# Menampilkan informasi dataset\nprint(dataset)","metadata":{"_uuid":"c11a5d52-2336-466d-ad02-ae45644010ab","_cell_guid":"45764441-acf3-4fdd-9fac-17f30ca0f493","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:30:58.368343Z","iopub.execute_input":"2023-12-13T12:30:58.368619Z","iopub.status.idle":"2023-12-13T12:30:58.395208Z","shell.execute_reply.started":"2023-12-13T12:30:58.368595Z","shell.execute_reply":"2023-12-13T12:30:58.394265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOAD MODEL & TOKENIZER","metadata":{"_uuid":"9639d871-56b3-48d7-923c-77ad7ec7dc47","_cell_guid":"b03fb8af-87dc-4366-a788-2de6d66e7010","trusted":true}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_id=\"google/flan-t5-base\"\n\n# Load tokenizer of FLAN-t5-base\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"_uuid":"83cb6a3a-7d43-41c3-aa6a-6ca2b108537e","_cell_guid":"be1a58a9-109b-4e66-8c2e-e972bb15807f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:30:58.396251Z","iopub.execute_input":"2023-12-13T12:30:58.396507Z","iopub.status.idle":"2023-12-13T12:30:58.548540Z","shell.execute_reply.started":"2023-12-13T12:30:58.396485Z","shell.execute_reply":"2023-12-13T12:30:58.547498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TOKENIZE","metadata":{"_uuid":"4e53879c-d02c-42e9-bd41-0b508bd27257","_cell_guid":"b5c20120-7086-430d-9f07-90b35f91a5fc","trusted":true}},{"cell_type":"code","source":"from datasets import concatenate_datasets\nfrom transformers import AutoTokenizer\n\n# The maximum total input sequence length after tokenization. \n# Sequences longer than this will be truncated, sequences shorter will be padded.\ntokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"RESPONSES\"], truncation=True), batched=True, remove_columns=['RESPONSES', 'LEVEL KOMPETENSI'])\nmax_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\nprint(f\"Max source length: {max_source_length}\")\n\n# The maximum total sequence length for target text after tokenization. \n# Sequences longer than this will be truncated, sequences shorter will be padded.\"\ntokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"LEVEL KOMPETENSI\"], truncation=True), batched=True, remove_columns=['RESPONSES', 'LEVEL KOMPETENSI'])\nmax_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\nprint(f\"Max target length: {max_target_length}\")","metadata":{"_uuid":"61f24082-d7b7-468a-b037-119ee1bbdd0b","_cell_guid":"a90b15a1-5933-40fe-a6b4-a15d39322a0c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:30:58.549666Z","iopub.execute_input":"2023-12-13T12:30:58.549953Z","iopub.status.idle":"2023-12-13T12:30:58.970983Z","shell.execute_reply.started":"2023-12-13T12:30:58.549929Z","shell.execute_reply":"2023-12-13T12:30:58.970100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PREPARED TRAIN","metadata":{"_uuid":"d368b537-9ea8-4573-b60f-c6da54ebed72","_cell_guid":"b1e6aa09-6127-4c55-9f88-d6b5e4d93111","trusted":true}},{"cell_type":"code","source":"# prefix = \"tolong klasifikasikan respon tersebut dimana masukan tersebut terdapat kelompok kompetensi dan respon dengan pemisah simbol semicolons, tujuan output prediksi klasifikasi respon tersebut terdapat pada level kompetensi berapa berdasarkan dari pola dataset training : \"\nprefix = \"terdapat input dengan format kelompok kompetensi dan respons yang dipisahkan dengan semicolon. klasifikasikan respons tersebut, berdasarkan kelompok kompetensi yang dimilikinya, responsnya masuk ke dalam level kompetensi berapa dalam rentang level 1 sampai 5 dan keluaran hanya berupa angka saja, responsnya adalah sebagai berikut : \"\n\ndef preprocess_function(sample, padding=\"max_length\"):\n    # add prefix to the input for t5\n    inputs = [prefix + item for item in sample[\"RESPONSES\"]]\n\n    # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=sample[\"LEVEL KOMPETENSI\"], max_length=max_target_length, padding=padding, truncation=True)\n    \n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"_uuid":"18addfb9-8982-4d24-bf8d-d039d9836ef2","_cell_guid":"43b22b6c-e054-408e-96b3-bfbd972210ba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:30:58.972298Z","iopub.execute_input":"2023-12-13T12:30:58.972593Z","iopub.status.idle":"2023-12-13T12:30:58.979852Z","shell.execute_reply.started":"2023-12-13T12:30:58.972568Z","shell.execute_reply":"2023-12-13T12:30:58.978956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## tokenize dataset with preprocess prefix\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['RESPONSES', 'LEVEL KOMPETENSI'])\nprint(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:30:58.981093Z","iopub.execute_input":"2023-12-13T12:30:58.981434Z","iopub.status.idle":"2023-12-13T12:30:59.450251Z","shell.execute_reply.started":"2023-12-13T12:30:58.981411Z","shell.execute_reply":"2023-12-13T12:30:59.449286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tune and evaluate FLAN-T5","metadata":{"_uuid":"d22608e2-29c8-4ba2-ad62-aa75b8b2818d","_cell_guid":"4b7c6df6-3206-46b6-9a0c-a525c7473040","trusted":true}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\n\n# load model from the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)","metadata":{"_uuid":"96fdb4a0-50b3-4962-ac1d-16974451c4c7","_cell_guid":"c5fdae5d-898a-48ed-a649-3c0d9dc0eb2d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:30:59.455129Z","iopub.execute_input":"2023-12-13T12:30:59.455422Z","iopub.status.idle":"2023-12-13T12:31:02.198424Z","shell.execute_reply.started":"2023-12-13T12:30:59.455397Z","shell.execute_reply":"2023-12-13T12:31:02.197627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport evaluate\n\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download(\"punkt\")\n\n# Metric\nmetric = evaluate.load(\"f1\")\n\n# helper function to postprocess text\n# post process convert token to label result\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\n# Fungsi ini mengembalikan evaluasi metrik yang dihitung, termasuk rata-rata skor F1 dan rata-rata durasi prediksi yang dihasilkan.\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, average='macro')\n    result = {k: round(v * 100, 4) for k, v in result.items()}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return result","metadata":{"_uuid":"45746a3c-f7e3-4a48-81d1-b9fb62f2ffce","_cell_guid":"0d90bd2a-d059-4cbd-8818-82b39bc4633a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:31:02.199743Z","iopub.execute_input":"2023-12-13T12:31:02.200046Z","iopub.status.idle":"2023-12-13T12:31:02.830435Z","shell.execute_reply.started":"2023-12-13T12:31:02.200003Z","shell.execute_reply":"2023-12-13T12:31:02.829482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# we want to ignore tokenizer pad token in the loss\nlabel_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"_uuid":"f7b002f8-5f47-43c5-9c03-2a7a1b5469dd","_cell_guid":"fe6e4381-dc0f-4779-98c6-567418932e83","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:31:02.831690Z","iopub.execute_input":"2023-12-13T12:31:02.832066Z","iopub.status.idle":"2023-12-13T12:31:02.837447Z","shell.execute_reply.started":"2023-12-13T12:31:02.832012Z","shell.execute_reply":"2023-12-13T12:31:02.836540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAINING ARGUMENTS","metadata":{}},{"cell_type":"code","source":"# Define local output directory\nlocal_output_dir = \"/kaggle/working/model_id\"  # Ganti dengan path direktori lokal yang diinginkan\n\n# Define training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=local_output_dir,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    fp16=False,  # Overflows with fp16\n    learning_rate=3e-4,\n\n    num_train_epochs=2,\n    # logging & evaluation strategies\n    logging_dir=f\"{local_output_dir}/logs\",\n    logging_strategy=\"epoch\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    load_best_model_at_end=False,\n    report_to=\"tensorboard\",\n    push_to_hub=False,  # Tidak push ke Hugging Face Hub\n)\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)","metadata":{"_uuid":"76f5ad16-90c4-4c92-9373-366cd71cbc0f","_cell_guid":"97e2dbdc-edcb-4f00-846f-ee08278ebbf9","jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2023-12-13T12:31:02.838615Z","iopub.execute_input":"2023-12-13T12:31:02.838909Z","iopub.status.idle":"2023-12-13T12:31:03.152754Z","shell.execute_reply.started":"2023-12-13T12:31:02.838886Z","shell.execute_reply":"2023-12-13T12:31:03.151634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n# Assuming you have already defined your model, data_collator, tokenized_dataset, and compute_metrics\n\n# Define local output directory\nlocal_output_dir = \"/kaggle/working/model_id\"  # Replace with the desired local directory path\n\n# Define training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=local_output_dir,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    fp16=False,  # Overflows with fp16\n    learning_rate=3e-4,\n    num_train_epochs=2,\n    # logging & evaluation strategies\n    logging_dir=f\"{local_output_dir}/logs\",\n    logging_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",  # Change to 'epoch' for evaluation at each epoch\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    load_best_model_at_end=False,\n    report_to=\"tensorboard\",\n    push_to_hub=False,  # Do not push to Hugging Face Hub\n)\n\n# Perform cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_index, test_index) in enumerate(kf.split(tokenized_dataset[\"train\"])):\n    print(f\"Fold {fold+1}/5\")\n\n    # Extract train and test datasets\n    train_dataset = tokenized_dataset[\"train\"].select(train_index)\n    test_dataset = tokenized_dataset[\"train\"].select(test_index)\n\n    # Create Trainer instance for each fold\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Evaluate the model\n    results = trainer.evaluate()\n\n    # Print or store the evaluation results as needed\n    print(results)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:57:05.259323Z","iopub.execute_input":"2023-12-13T12:57:05.260007Z","iopub.status.idle":"2023-12-13T13:08:01.572297Z","shell.execute_reply.started":"2023-12-13T12:57:05.259971Z","shell.execute_reply":"2023-12-13T13:08:01.571217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-12-13T14:48:04.257730Z","iopub.execute_input":"2023-12-13T14:48:04.258634Z","iopub.status.idle":"2023-12-13T14:48:04.262611Z","shell.execute_reply.started":"2023-12-13T14:48:04.258599Z","shell.execute_reply":"2023-12-13T14:48:04.261698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nfrom sklearn.metrics import classification_report\n\n# Assuming you have already defined your model, data_collator, tokenized_dataset, and compute_metrics\n\n# Define local output directory\nlocal_output_dir = \"/kaggle/working/model_id\"  # Replace with the desired local directory path\n\n# Define training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=local_output_dir,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    fp16=False,\n    learning_rate=3e-4,\n    num_train_epochs=2,\n    logging_dir=f\"{local_output_dir}/logs\",\n    logging_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    load_best_model_at_end=False,\n    report_to=\"tensorboard\",\n    push_to_hub=False,\n)\n\n# Perform cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_index, test_index) in enumerate(kf.split(tokenized_dataset[\"train\"])):\n    print(f\"Fold {fold+1}/5\")\n\n    # Extract train and test datasets\n    train_dataset = tokenized_dataset[\"train\"].select(train_index)\n    test_dataset = tokenized_dataset[\"train\"].select(test_index)\n\n    # Create Trainer instance for each fold\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    # Train the model\n    trainer.train()\n    \n    samples_number = len(dataset['test'])\n    progress_bar = tqdm(range(samples_number))\n    predictions_list = []\n    labels_list = []\n    for i in range(samples_number):\n      text = dataset['test']['RESPONSES'][i]\n      inputs = tokenizer.encode_plus(text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n      outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150, num_beams=4, early_stopping=True)\n      prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n      predictions_list.append(prediction)\n      labels_list.append(dataset['test']['LEVEL KOMPETENSI'][i])\n\n      progress_bar.update(1)\n    \n    \n    str_labels_list = []\n    for i in range(len(labels_list)): str_labels_list.append(str(labels_list[i]))\n        \n    report = classification_report(str_labels_list, predictions_list, zero_division=0)\n    print(report)\n\n#     # Evaluate the model\n#     results = trainer.evaluate()\n\n#     # Print or store the evaluation results as needed\n#     print(results)\n\n#     # Additional: Print or store the evaluation metrics\n#     print(f\"Fold {fold+1} Evaluation Metrics:\")\n#     for key, value in results.items():\n#         print(f\"{key}: {value}\")\n\n#     # Print a separator for better readability\n#     print(\"=\" * 50)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T14:48:07.957253Z","iopub.execute_input":"2023-12-13T14:48:07.958201Z","iopub.status.idle":"2023-12-13T15:00:01.246969Z","shell.execute_reply.started":"2023-12-13T14:48:07.958167Z","shell.execute_reply":"2023-12-13T15:00:01.246056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## START TRAIN","metadata":{}},{"cell_type":"code","source":"# Start training \ntrainer.train()","metadata":{"_uuid":"fe1c455e-506e-408d-9243-e3aa5cad0fa4","_cell_guid":"e878a2e1-25ce-44ed-bb39-0d9548b3fda3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:31:03.228542Z","iopub.status.idle":"2023-12-13T12:31:03.228935Z","shell.execute_reply.started":"2023-12-13T12:31:03.228731Z","shell.execute_reply":"2023-12-13T12:31:03.228749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# evaluate model\ntrainer.evaluate()","metadata":{"_uuid":"e823d183-4f2c-4055-87e4-776f5b82cda7","_cell_guid":"19ada7a3-0667-4e73-a077-aa12dd8105b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:31:03.230054Z","iopub.status.idle":"2023-12-13T12:31:03.230689Z","shell.execute_reply.started":"2023-12-13T12:31:03.230471Z","shell.execute_reply":"2023-12-13T12:31:03.230498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SAVE MODEL","metadata":{}},{"cell_type":"code","source":"# Save the trained model and tokenizer locally\nlocal_model_dir = \"/kaggle/working/ourmodel\"\nmodel.save_pretrained(local_model_dir)\ntokenizer.save_pretrained(local_model_dir)","metadata":{"_uuid":"7b10148f-0f77-46fe-bc62-1523da1f80f3","_cell_guid":"ce4345cc-680d-42ed-a721-12fbb9aa83be","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:31:03.232301Z","iopub.status.idle":"2023-12-13T12:31:03.232657Z","shell.execute_reply.started":"2023-12-13T12:31:03.232460Z","shell.execute_reply":"2023-12-13T12:31:03.232475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.233572Z","iopub.status.idle":"2023-12-13T12:31:03.233937Z","shell.execute_reply.started":"2023-12-13T12:31:03.233773Z","shell.execute_reply":"2023-12-13T12:31:03.233789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.svm import SVC\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import StratifiedKFold\n\n# Load pre-trained tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/flan-t5-base\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"flax-community/flan-t5-base\")\n\n\n# # Assuming df_train is your training DataFrame with 'text' and 'label' columns\n# texts = df_train['train'].tolist()\n# labels = df_train['test'].tolist()\n\n# # Tokenize the data\n# encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n# labels = torch.tensor(labels)\n\n# # Create DataLoader\n# dataset = TensorDataset(encodings.input_ids, encodings.attention_mask, labels)\n# loader = DataLoader(dataset, batch_size=8, shuffle=True)\n\n# # Define the SVM classifier\n# clf = SVC(kernel='linear', C=1, random_state=0)\n\n# # Perform cross-validation\n# scoring = {'precision_macro', 'recall_macro'}\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# scores = cross_val_score(clf, X=encodings.input_ids, y=labels, cv=cv, scoring=make_scorer(recall_score, average='macro'))\n# print(\"Cross-validated recall scores:\", scores)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.235148Z","iopub.status.idle":"2023-12-13T12:31:03.235495Z","shell.execute_reply.started":"2023-12-13T12:31:03.235315Z","shell.execute_reply":"2023-12-13T12:31:03.235331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Simpan model dan tokenizer menggunakan pickle\nwith open(\"t5_model.pkl\", \"wb\") as model_file:\n    pickle.dump(model, model_file)\n\nwith open(\"t5_tokenizer.pkl\", \"wb\") as tokenizer_file:\n    pickle.dump(tokenizer, tokenizer_file)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.236847Z","iopub.status.idle":"2023-12-13T12:31:03.237228Z","shell.execute_reply.started":"2023-12-13T12:31:03.237001Z","shell.execute_reply":"2023-12-13T12:31:03.237033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference and Classification Report","metadata":{"_uuid":"fcdda747-d29d-4efa-abf8-da3f17c01f35","_cell_guid":"02ee35f7-a165-4681-b4ac-2efc554a6b94","trusted":true}},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nsamples_number = len(dataset['test'])\nprogress_bar = tqdm(range(samples_number))\npredictions_list = []\nlabels_list = []\nfor i in range(samples_number):\n  text = dataset['test']['RESPONSES'][i]\n  inputs = tokenizer.encode_plus(text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n  outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150, num_beams=4, early_stopping=True)\n  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  predictions_list.append(prediction)\n  labels_list.append(dataset['test']['LEVEL KOMPETENSI'][i])\n\n  progress_bar.update(1)","metadata":{"_uuid":"b8beefcf-44cb-4af0-a864-3bea7e510f98","_cell_guid":"6a7cf966-9b89-4a18-8164-2e78370a3584","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:31:03.238325Z","iopub.status.idle":"2023-12-13T12:31:03.238668Z","shell.execute_reply.started":"2023-12-13T12:31:03.238476Z","shell.execute_reply":"2023-12-13T12:31:03.238490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"str_labels_list = []\nfor i in range(len(labels_list)): str_labels_list.append(str(labels_list[i]))","metadata":{"_uuid":"bdf3af78-58f7-43aa-b3c0-8d754223ce4b","_cell_guid":"c04de030-0d3f-4c96-bc63-44cbe94bbeed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-13T12:31:03.240228Z","iopub.status.idle":"2023-12-13T12:31:03.240544Z","shell.execute_reply.started":"2023-12-13T12:31:03.240383Z","shell.execute_reply":"2023-12-13T12:31:03.240397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nreport = classification_report(str_labels_list, predictions_list, zero_division=0)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.241975Z","iopub.status.idle":"2023-12-13T12:31:03.242325Z","shell.execute_reply.started":"2023-12-13T12:31:03.242168Z","shell.execute_reply":"2023-12-13T12:31:03.242184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IF YOU WANT MODEL ACCELERATOR BY CPU","metadata":{}},{"cell_type":"code","source":"import torch\n\n# move model & tensor to GPU cuda\nloaded_model = model.to('cpu')\ninputs = inputs.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.243472Z","iopub.status.idle":"2023-12-13T12:31:03.243776Z","shell.execute_reply.started":"2023-12-13T12:31:03.243624Z","shell.execute_reply":"2023-12-13T12:31:03.243638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the text you want to generate predictions\ninput_text = \"Pada saat saya menjabat sebagai Camat Cipaku saya mengadakan kegiatan pengajian syukuran dengan mengundang tetangga dan sanak keluarga dirumah ketika pengajian akan dimulai saya mendapatkan informasi terjadinya bencana alam angin puting beliung di Desa Bangbayang Kecamatan Cipaku sehingga saya langsung meminta maaf kepada tamu undangan karena saya tidak dapat mengikuti kegiatan tersebut hingga selesai karena saya selaku camat harus berada pada lokasi bencana untuk segera mengambil keputusan dan koordinasi terkait evakuasi warga yang terkena dampak bencana Jadi saya akan menghadapi dengan tenang dan saya akan memprioritaskan penyelesaian konflik kepentingan pekerjaan dengan mengidentifikasi permasalahan mengdiagnosis permasalahan yang terjadi memberikan solusi pelaksaan solusi dan mengevaluasi solusi tersebut Setelah menyelesaikan koflik pekerjaan selanjutnya adalah menyelesaikan konflik kepentingan pribadi\"\n\n# Tokenize and generate predictions\ninputs = tokenizer.encode_plus(input_text, padding='max_length', max_length=512, return_tensors='pt').to('cpu')\noutputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150, num_beams=4, early_stopping=True)\nprediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the prediction\nprint(\"Input Text:\", input_text, \"\\n\")\nprint(\"Prediction:\", prediction)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.245697Z","iopub.status.idle":"2023-12-13T12:31:03.246176Z","shell.execute_reply.started":"2023-12-13T12:31:03.245910Z","shell.execute_reply":"2023-12-13T12:31:03.245932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IF YOU WANT ACCELERATOR BY GPU NVIDIA CUDA","metadata":{}},{"cell_type":"code","source":"import torch\n\n# move model & tensor to GPU cuda\nloaded_model = model.to('cuda')\ninputs = inputs.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.247661Z","iopub.status.idle":"2023-12-13T12:31:03.248160Z","shell.execute_reply.started":"2023-12-13T12:31:03.247876Z","shell.execute_reply":"2023-12-13T12:31:03.247898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the text you want to generate predictions\ninput_text = \"Saya belum pernah mengalami kepentingan konflik yang bersifat ekstrim terkait keluarga, misalnya orang tua sakit keras dan memerlukan dijenguk sesegera mungkin ketika saya sedang memiliki pekerjaan yang harus sesegera mungkin diselesaikan. Padahal sebagai seorang ASN telah terikat dalam sumpah yang harus mendahulukan kepentingan umum dibandingkan kepentingan pribadi atau golongan. Bilamana hal itu terjadi maka upaya yang akan dilakukan adalah meminta izin kepada atasan langsung dan meminta bantuan rekan kerja untuk menangani sementara pekerjaan tersebut di atas dengan terlebih dahulu memberikan penjelasan perihal pekerjaan dimaksud. Namun demikian di sisi lain sebagai bagian dari penyelenggara negara, konflik kepentingan pribadi dengan pekerjaan bila tidak dikelola dengan baik, salah satunya akan mendorong ke arah prilaku koruptif dan prilaku menyimpang lainnya dengan memanfaatkan fasilitas jabatan, kelemahan sistem ataupun keinginan untuk memperkaya diri sendiri atau orang lain. Oleh karena itu upaya pencegahan yang paling efektif adalah dengan tetap berpegang teguh pada sumpah ketika dilantik sebagai seorang PNS pegawai dan ketika dilantik sebagai seorang pemegang jabatan. Bahwa sumpah tersebut disamping disaksikan oleh manusia, juga disaksikan oleh Allah SWT  akan dihisab  dikemudian hari kelak , serta memperhatikan perundang  undangan dan kebijakan yang berlaku.\"\n# Tokenize and generate predictions\ninputs = tokenizer.encode_plus(input_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\noutputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150, num_beams=4, early_stopping=True)\nprediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the prediction\nprint(\"Input Text:\", input_text, \"\\n\")\nprint(\"Prediction:\", prediction)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.249211Z","iopub.status.idle":"2023-12-13T12:31:03.249651Z","shell.execute_reply.started":"2023-12-13T12:31:03.249423Z","shell.execute_reply":"2023-12-13T12:31:03.249445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.iloc[134]","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.251886Z","iopub.status.idle":"2023-12-13T12:31:03.252246Z","shell.execute_reply.started":"2023-12-13T12:31:03.252078Z","shell.execute_reply":"2023-12-13T12:31:03.252095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"t5_model.pkl\", \"rb\") as model_file:\n    loaded_model = pickle.load(model_file)\n\nwith open(\"t5_tokenizer.pkl\", \"rb\") as tokenizer_file:\n    loaded_tokenizer = pickle.load(tokenizer_file)\n    \n# Define the text you want to generate predictions\ninput_text = \"Saya belum pernah mengalami kepentingan konflik yang bersifat ekstrim terkait keluarga, misalnya orang tua sakit keras dan memerlukan dijenguk sesegera mungkin ketika saya sedang memiliki pekerjaan yang harus sesegera mungkin diselesaikan. Padahal sebagai seorang ASN telah terikat dalam sumpah yang harus mendahulukan kepentingan umum dibandingkan kepentingan pribadi atau golongan. Bilamana hal itu terjadi maka upaya yang akan dilakukan adalah meminta izin kepada atasan langsung dan meminta bantuan rekan kerja untuk menangani sementara pekerjaan tersebut di atas dengan terlebih dahulu memberikan penjelasan perihal pekerjaan dimaksud. Namun demikian di sisi lain sebagai bagian dari penyelenggara negara, konflik kepentingan pribadi dengan pekerjaan bila tidak dikelola dengan baik, salah satunya akan mendorong ke arah prilaku koruptif dan prilaku menyimpang lainnya dengan memanfaatkan fasilitas jabatan, kelemahan sistem ataupun keinginan untuk memperkaya diri sendiri atau orang lain. Oleh karena itu upaya pencegahan yang paling efektif adalah dengan tetap berpegang teguh pada sumpah ketika dilantik sebagai seorang PNS pegawai dan ketika dilantik sebagai seorang pemegang jabatan. Bahwa sumpah tersebut disamping disaksikan oleh manusia, juga disaksikan oleh Allah SWT  akan dihisab  dikemudian hari kelak , serta memperhatikan perundang  undangan dan kebijakan yang berlaku.\"\n# Tokenize and generate predictions\ninputs = loaded_tokenizer.encode_plus(input_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\noutputs = loaded_model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150, num_beams=4, early_stopping=True)\nprediction = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the prediction\nprint(\"Input Text:\", input_text, \"\\n\")\nprint(\"Prediction:\", prediction)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T12:31:03.253317Z","iopub.status.idle":"2023-12-13T12:31:03.253653Z","shell.execute_reply.started":"2023-12-13T12:31:03.253486Z","shell.execute_reply":"2023-12-13T12:31:03.253502Z"},"trusted":true},"execution_count":null,"outputs":[]}]}